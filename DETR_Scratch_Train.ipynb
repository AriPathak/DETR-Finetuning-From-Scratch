{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWbHqwrIvl5A"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as T\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torchvision.io import read_image\n",
        "from torchvision.models._utils import IntermediateLayerGetter\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from transformers import DetrForObjectDetection\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from torchvision import tv_tensors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit To: https://www.kaggle.com/code/bibhasmondal96/detr-from-scratch/notebook"
      ],
      "metadata": {
        "id": "C6cu9QcJ4Psy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip -P data\n",
        "!cd data && unzip PennFudanPed.zip\n",
        "\n",
        "image = read_image(\"data/PennFudanPed/PNGImages/FudanPed00046.png\")\n",
        "mask = read_image(\"data/PennFudanPed/PedMasks/FudanPed00046_mask.png\")\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.subplot(122)\n",
        "plt.title(\"Mask\")\n",
        "plt.imshow(mask.permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "MdPLM6keyD7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = read_image(img_path)\n",
        "        mask = read_image(mask_path)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = torch.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "        num_objs = len(obj_ids)\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        boxes = masks_to_boxes(masks)\n",
        "\n",
        "        # there is only one class\n",
        "        labels = {'class_labels' : torch.zeros((num_objs,), dtype=torch.int64)}\n",
        "\n",
        "        image_id = idx\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        # Wrap sample and targets into torchvision tv_tensors:\n",
        "        img = torch.tensor(img)\n",
        "\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
        "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "            target['boxes'] = self.transforms(target['boxes'])\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "metadata": {
        "id": "jFV4d5yJyGmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = PennFudanDataset('data/PennFudanPed', None)\n",
        "def create_loader(d, start, end, batch_size, shuffle=False):\n",
        "  data = []\n",
        "  for i in range(start, end):\n",
        "    sel = d[i]\n",
        "    data.append({\"pixel_values\" : sel[0].cpu()/255.0, \"pixel_mask\" : sel[1]['boxes'].cpu()/255.0, \"labels\" : sel[1]['labels']})\n",
        "  return DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "train_loader = create_loader(dataset, 0, int(len(dataset)*0.85), 1, shuffle=True)\n",
        "test_loader = create_loader(dataset, int(len(dataset)*0.85), int(len(dataset)), 1, shuffle=False)"
      ],
      "metadata": {
        "id": "LQFXFqkQyL4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "6R7gPB853n3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BoxUtils(object):\n",
        "    @staticmethod\n",
        "    def box_cxcywh_to_xyxy(x):\n",
        "        x_c, y_c, w, h = x.unbind(-1)\n",
        "        b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "             (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "        return torch.stack(b, dim=-1)\n",
        "\n",
        "    @staticmethod\n",
        "    def box_xyxy_to_cxcywh(x):\n",
        "        x0, y0, x1, y1 = x.unbind(-1)\n",
        "        b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
        "             (x1 - x0), (y1 - y0)]\n",
        "        return torch.stack(b, dim=-1)\n",
        "\n",
        "    @staticmethod\n",
        "    def rescale_bboxes(out_bbox, size):\n",
        "        img_h, img_w = size\n",
        "        b = BoxUtils.box_cxcywh_to_xyxy(out_bbox)\n",
        "        b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "        return b\n",
        "\n",
        "    @staticmethod\n",
        "    def box_area(boxes):\n",
        "        \"\"\"\n",
        "        Computes the area of a set of bounding boxes, which are specified by its\n",
        "        (x1, y1, x2, y2) coordinates.\n",
        "        Arguments:\n",
        "            boxes (Tensor[N, 4]): boxes for which the area will be computed. They\n",
        "                are expected to be in (x1, y1, x2, y2) format\n",
        "        Returns:\n",
        "            area (Tensor[N]): area for each box\n",
        "        \"\"\"\n",
        "        return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "\n",
        "    @staticmethod\n",
        "    # modified from torchvision to also return the union\n",
        "    def box_iou(boxes1, boxes2):\n",
        "        area1 = BoxUtils.box_area(boxes1)\n",
        "        area2 = BoxUtils.box_area(boxes2)\n",
        "\n",
        "        lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
        "        rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
        "\n",
        "        wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "        inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
        "\n",
        "        union = area1[:, None] + area2 - inter\n",
        "\n",
        "        iou = inter / union\n",
        "        return iou, union\n",
        "\n",
        "    @staticmethod\n",
        "    def generalized_box_iou(boxes1, boxes2): #Bug\n",
        "        \"\"\"\n",
        "        Generalized IoU from https://giou.stanford.edu/\n",
        "        The boxes should be in [x0, y0, x1, y1] format\n",
        "        Returns a [N, M] pairwise matrix, where N = len(boxes1)\n",
        "        and M = len(boxes2)\n",
        "        \"\"\"\n",
        "        # degenerate boxes gives inf / nan results\n",
        "        # so do an early check\n",
        "\n",
        "        iou, union = BoxUtils.box_iou(boxes1, boxes2)\n",
        "\n",
        "        lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
        "        rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
        "\n",
        "        wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "        area = wh[:, :, 0] * wh[:, :, 1]\n",
        "\n",
        "        return iou - (area - union) / area"
      ],
      "metadata": {
        "id": "ltvGrH1uy2Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HungarianMatcher(nn.Module):\n",
        "\n",
        "    def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 1):\n",
        "        super().__init__()\n",
        "        self.cost_class = cost_class\n",
        "        self.cost_bbox = cost_bbox\n",
        "        self.cost_giou = cost_giou\n",
        "        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, targets):\n",
        "        bs, num_queries = outputs[\"logits\"].shape[:2]\n",
        "\n",
        "        # We flatten to compute the cost matrices in a batch\n",
        "        out_prob = outputs[\"logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
        "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
        "\n",
        "        # Also concat the target labels and boxes\n",
        "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
        "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
        "\n",
        "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
        "        # but approximate it in 1 - proba[target class].\n",
        "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
        "        cost_class = -out_prob[:, tgt_ids]\n",
        "\n",
        "        # Compute the L1 cost between boxes\n",
        "        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
        "\n",
        "        # Compute the giou cost betwen boxes\n",
        "        cost_giou = -BoxUtils.generalized_box_iou(\n",
        "            BoxUtils.box_cxcywh_to_xyxy(out_bbox),\n",
        "            BoxUtils.box_cxcywh_to_xyxy(tgt_bbox)\n",
        "        )\n",
        "\n",
        "        # Final cost matrix\n",
        "        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n",
        "        C = C.view(bs, num_queries, -1).cpu()\n",
        "\n",
        "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
        "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
        "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
        "\n",
        "class SetCriterion(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses, device='cuda:0'):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.device = device\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.eos_coef = eos_coef\n",
        "        self.losses = losses\n",
        "        empty_weight = torch.ones(self.num_classes + 1)\n",
        "        empty_weight[-1] = self.eos_coef\n",
        "        self.register_buffer('empty_weight', empty_weight)\n",
        "\n",
        "    def loss_labels(self, outputs, targets, indices, num_boxes):\n",
        "        assert 'logits' in outputs\n",
        "        src_logits = outputs['logits']\n",
        "\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
        "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
        "                                    dtype=torch.int64, device=src_logits.device)\n",
        "        target_classes[idx] = target_classes_o\n",
        "        ew = self.empty_weight\n",
        "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, ew.cuda())\n",
        "        losses = {'loss_ce': loss_ce}\n",
        "        return losses\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n",
        "        pred_logits = outputs['logits']\n",
        "        device = pred_logits.device\n",
        "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
        "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
        "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
        "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
        "        losses = {'cardinality_error': card_err}\n",
        "        return losses\n",
        "\n",
        "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
        "        assert 'pred_boxes' in outputs\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        src_boxes = outputs['pred_boxes'][idx]\n",
        "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
        "\n",
        "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
        "\n",
        "        losses = {}\n",
        "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
        "\n",
        "        loss_giou = 1 - torch.diag(BoxUtils.generalized_box_iou(\n",
        "            BoxUtils.box_cxcywh_to_xyxy(src_boxes),\n",
        "            BoxUtils.box_cxcywh_to_xyxy(target_boxes))\n",
        "        )\n",
        "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
        "        return losses\n",
        "\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        # permute predictions following indices\n",
        "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _get_tgt_permutation_idx(self, indices):\n",
        "        # permute targets following indices\n",
        "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
        "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
        "        return batch_idx, tgt_idx\n",
        "\n",
        "    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n",
        "        loss_map = {\n",
        "            'labels': self.loss_labels,\n",
        "            'cardinality': self.loss_cardinality,\n",
        "            'boxes': self.loss_boxes,\n",
        "        }\n",
        "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
        "        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
        "\n",
        "        # Retrieve the matching between the outputs of the last layer and the targets\n",
        "        indices = self.matcher(outputs_without_aux, targets)\n",
        "\n",
        "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
        "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
        "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
        "\n",
        "        # Compute all the requested losses\n",
        "        losses = {}\n",
        "        for loss in self.losses:\n",
        "          indices[0][0].to(self.device)\n",
        "          indices[0][1].to(self.device)\n",
        "\n",
        "          losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n",
        "\n",
        "        return losses"
      ],
      "metadata": {
        "id": "a0LUc97sy6iC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detr = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\",\n",
        "            num_labels=1,\n",
        "            ignore_mismatched_sizes=True, revision=\"no_timm\")"
      ],
      "metadata": {
        "id": "bKs7qMTtxaVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts"
      ],
      "metadata": {
        "id": "p2dt2b0E2jIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = HungarianMatcher(cost_class = 1, cost_bbox = 5, cost_giou = 2)\n",
        "weight_dict = {'loss_ce': 1, 'loss_bbox': 5, 'loss_giou': 2}\n",
        "losses = ['labels', 'boxes', 'cardinality']\n",
        "criterion = SetCriterion(1, matcher=matcher, weight_dict=weight_dict, eos_coef=0.1, losses=losses, device='cpu')\n",
        "optimizer = torch.optim.Adam(detr.parameters(), lr=1e-4)\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
        "num_epochs = 100"
      ],
      "metadata": {
        "id": "0Ex0EkHfzR6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oY3h_TsYy1p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, train_loader, optimizer, criterion, scheduler=None, device='cuda:0'):\n",
        "  epoch_loss = 0\n",
        "  for batch in tqdm(train_loader, leave=False):\n",
        "    image = batch['pixel_values'].to(device)\n",
        "    mask = batch['pixel_mask'].to(device)\n",
        "    label = batch['labels']['class_labels'].to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(image)\n",
        "    crit_batch = [{'boxes' : mask.squeeze(0), 'labels' : label.squeeze(0)}]\n",
        "    #for t in outputs:\n",
        "    #  outputs[f\"{t}\"] = outputs[f\"{t}\"].to('cpu')\n",
        "    loss = criterion(outputs, crit_batch)\n",
        "    loss = sum(loss[k] * weight_dict[k] for k in loss.keys() if k in weight_dict)\n",
        "    epoch_loss += loss.detach().cpu().item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if scheduler:\n",
        "      scheduler.step()\n",
        "  return epoch_loss / len(train_loader)\n",
        "\n",
        "#TODO: add valid loader and valid iou accuracy function\n",
        "def engine(model, epochs, train_loader, criterion, optimizer, scheduler=None, device='cuda:0'):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    epochs_loss = train_one_epoch(model, train_loader, optimizer, criterion, scheduler, device=device)\n",
        "    print(f\"Epoch {epoch} loss: {epochs_loss}\")"
      ],
      "metadata": {
        "id": "0rAaq10oxb7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engine(detr, num_epochs, train_loader, criterion, optimizer, scheduler=scheduler, device='cuda:0')"
      ],
      "metadata": {
        "id": "ZdTBQRnizdXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"DETR_Scratch_Trial0\"\n",
        "torch.save(detr.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "elNktgzg1RCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.patches as patches\n",
        "def display(image, output):\n",
        "  fig, ax = plt.subplots()\n",
        "  rectangles = []\n",
        "  boxes = output['pred_boxes']\n",
        "  image = image.cpu()\n",
        "  for coords in boxes.squeeze(0):\n",
        "    a = coords*255.0\n",
        "    rectangles.append(patches.Rectangle((a[0], a[1]), a[2]-a[0], a[3]-a[1], linewidth=1, edgecolor='r', facecolor='none'))\n",
        "\n",
        "  ax.imshow(image.squeeze().permute(1, 2, 0))\n",
        "\n",
        "  if rectangles is not None:\n",
        "    for rec in rectangles:\n",
        "      ax.add_patch(rec)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "VmA8opDT1yVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def inference(test_data, model):\n",
        "  model.eval()\n",
        "  model.cpu()\n",
        "  for batch in test_data:\n",
        "    image = batch[\"pixel_values\"]\n",
        "    mask = batch['pixel_mask']\n",
        "    output = model(image)\n",
        "    display(image, output)"
      ],
      "metadata": {
        "id": "mNJRxpi51QCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference(test_loader, detr)"
      ],
      "metadata": {
        "id": "YtG8f0i82CgS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}